Datasets for Periocular Images

UBIRIS V2 - Need to Apply
UBIPR - Got the database
IITD Multispectral (IMP) - Need to apply
MobBio
FRGC face dataset - provides Gender and Ethnicity - Need to apply
CROSS EYED dataset - Applied already





Hallucinating the Full Face from the Periocular Region via Dimensionally Weighted K-SVD




How about using a masked dataset to capture only the periocular area and then generate the facial image, the attributes generated using the periocular region could also be used for face generation.





 The CROSS-EYED, [3] - ”Reading Cross-spectrum Iris/Periocular Dataset,” is a benchmark dataset for the identification competition presented in BTAS 2016. It is composed of two eyes images in both visible (VIS/RGB) and NIR. The images are acquired from a distance of around 1.5 m. The images acquired under NIR wavelength have a single channel, while the visible spectrum iris images contain three channels of information. The images present a realistic indoor environment with a realistic illumination condition, and there are large variations in the ethnicity and eye color as well as realistic and challenging illumination reflections. The database is constructed with 120 subjects. Each subject is composed of two additional folders, one NIR and one VIS. For each subject, there are eight images (960 left and 960 right, 1,920 images per spectrum, 3,840 in total), as demonstrated by 632 males and 328 females images with 18 females subject with visual evidence of make-up. See Figure 1. [Gender Classification from Multispectral Periocular Images]


FRGC:The visible spectrum periocular images are obtained from high resolution frontal face images belonging to the FRGC dataset [16] that are captured under different conditions. The high resolution still face images (≈ 1200×1400, 72 dpi) allow for the periocular       texture to be imaged in significant detail. Also, the ground truth eye centers are provided, making it easier to crop out the periocular images and scale them to the required size. In this work, we scale the cropped periocular images to the uniform size of 251 × 251 pixels. The distance of the subject from the camera is assumed to be constant for controlled settings, hence the effects of scale change are assumed to be negligible. ((Soft Biometric Classification Using Periocular Region Features))


FRGC: the Face Recognition Grand Challenge [16] (FRGC) set, released by the National Institute of Standards and Technology (NIST). Again, all the 24,946 RGB samples in this set (with periocular regions cropped and resized into 150 × 200 × 3 pixels) were considered.
Cropping the left/right eye regions from each image yields a total of 894 classes.


 The databases used in this study are listed in Table II for NIR spectrum and Table III for VW, respectively. Sample images of NIR as well as VW ocular images are shown in Fig. 2. Note, that the amount of periocular information present in the processed images may vary. Also the spectral band may vary for NIR images. Since images are directly used for the task of sex-prediction all images are referred to as periocular images. While NIR images are mostly acquired under more constrained conditions, VW images show higher variations in
the capture process. For some of the employed databases, e.g. GFI UND or UND VAL, sex information is available. For others, e.g. CROSS-EYED or UTIRIS, sex information is available upon request. Eventually, remaining databases,
e.g. CASIA-DISTANCE or MOBBIO, were manually labeled based on (corresponding) face images. In case only face images were available within databases, e.g. CASIA-DISTANCE or MICHE (iPhone 5), the OpenCV 2.10 eye detector was
employed to automatically detect and crop the left and right ocular regions. Images where the eye detector failed were deleted from the database. Another alternative to obtain a sufficiently large database of periocular images with sex-labels
would be to process further face databases for which these labels are available (Sex-Prediction from Periocular Images across Multiple Sensors and Spectra) 




UBIPR: This database contains 5,126 left and 5,126 right periocular images from 344 subjects, and simulates less constrained periocular acquisition environment under visible spectrum. Noticeable amount of images from this dataset present occlusion, off-angle or illumination variation. For the experiments, only left periocular images are used. We employed the same training set of 3,359 images as used in [9] for model learning. The remaining 1,767 left images are used for test phase for performance evaluation. This database
is used for open-world experiments and therefore no subjects are overlapping between the training and test sets. [Improving Periocular Recognition by Explicit Attention to Critical Regions in Deep Neural Network]

 
 
 
 Papers using different datasets:
 
 1. UBIPr : Gender information available
 a. Improving Periocular Recognition by Explicit Attention to Critical Regions in Deep Neural Network : Only left eye with 5126 images corresponding to 344 subjects, 3359 for train and 1767 for test, open world experiments.
 b. Periocular Recognition using Unsupervised Convolutional RBM Feature Learning : 10,252 images for 344 subjects, 44 for train and 300 for test.
 c. Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization
 d. Accurate Periocular Recognition Under Less Constrained Environment Using Semantics-Assisted Convolutional Neural Network : This paper uses verification rate vs FAR (ROC curve) and
 also CMC curve for identification scenario.
 
 
 2. FRGC: Gender and ethnicity available
 a. Soft Biometric Classification Using Periocular Region Features This paper has used 251x251 image size
 b. Improving Periocular Recognition by Explicit Attention to Critical Regions in Deep Neural Network
 c. Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks Image size 150 × 200 × 3
 d. Accurate Periocular Recognition Under Less Constrained Environment Using Semantics-Assisted Convolutional Neural Network
 e. Towards Online Iris and Periocular Recognition Under Relaxed Imaging Constraints : The subset images were selected from the session 2002–269 to 2002–317 of Fall 2002
 
 3. CROSS-EYed: Gender information available upon request
 a. Gender Classification from Multispectral Periocular Images
 b. Sex-Prediction from Periocular Images across Multiple Sensors and Spectra

 Preprocessing

 FRGC dataset:
'/home/n-lab/Documents/Periocular_project/Datasets/FRGC/BEE_DIST/FRGC1.0/metadata' contains the information for left eye center and right eye center for the images.

Try to find how many images are there in the Spring 2004range folder located at '/home/n-lab/Documents/Periocular_project/Datasets/FRGC/FRGC-2.0-dist/nd1/Spring2004range'.
This folder has close-up images that may be easier for getting the periocular regions
The folder Spring 2004 has some images that are a little close-ups but not a whole lot. May need to detect faces and then detect the periocular region
The folder 2002 is really good images 2 close-ups and 2 distant images. May be I should work with Fall 2002 images and try to crop the face and periocular regions

The folder Spring 2004 range contains about 2114 images in total. These images are close-up and seem to be good.
The folder Spring 2004 contains about 12684 images, out of which about 8456 could be useful.
The folder Spring 2003 contains about 11334 images, out of which about 6600 could be useful.
The folder Fall 2003 has 11358 images, out of which about which 7600 could be useful

Based on previous papers, I have just taken the Fall 2002 dataset from 202-269 to 202-317 images for our calulation.
First I copied the folders from 202-269 to 317 to the folder '/home/n-lab/Documents/Periocular_project/Datasets/FRGC_dataset_extracted_periocular_images/Fall2002_Images_periocular'
Then I cropped the periocular region using the script face_eye_detector_FRGC.py and stored the results in 'Fall2002_Images_cropped_periocular'. Next I deleted some of the bad detections
that had like nose images, hair images. There were a lot of images and I could only clean images for subjects 02463-04430. All these clean images were stored in folder 'Fall2002_Images_copped_periocular_cleaned'

Next I used the function create_class_folders from the script pre_processing_FRGC to create the class folders to be used for classification network.
The Class folders were saved at '/home/n-lab/Documents/Periocular_project/Datasets/FRGC_dataset_extracted_periocular_images/Class_Folders'
Total number of classes was about 110. Each subject had different number of images with a minimum of 4 images per subject to a max of 41 images per subject.

FRGC_Spring 2004 Dataset:

I decided to train the model only on FRGC Spring 2004 dataset which has about 12684 images.
I used the MTCNN detector to find the eye keypoints. I used the function run_no_subfolders from the script mtcnn_detector_pytorch.py for the mtcnn detector.
The FRGC Spring 2004 dataset used is located at '/home/n-lab/Documents/Periocular_project/Datasets/FRGC/FRGC-2.0-dist/nd1/Spring2004'.
Next using the above script, the left eye keypoint and right eye keypoint were detected the left periocular region and right periocular region were cropped and saved at
'/home/n-lab/Documents/Periocular_project/Datasets/FRGC_dataset_extracted_periocular_images_MTCNN/Spring2004_cropped_Left_Eye' and
'/home/n-lab/Documents/Periocular_project/Datasets/FRGC_dataset_extracted_periocular_images_MTCNN/Spring2004_cropped_Right_Eye' respectively.
Next, I removed some of the non-periocular region image crops using the function rem_files from the script pre_processing_FRGC.
Next I used the function create_class_folders from the script pre_processing_FRGC to create the class folders to be used for classification network.
The Class folders were saved at '/home/n-lab/Documents/Periocular_project/Datasets/FRGC_dataset_extracted_periocular_images_MTCNN/Spring2004_cropped_Class_Folders'
Total number of classes was about 690.



UBIPr - 10199 images There were a few images that were not clear and had issues, so we removed those images and we got only 10138
Total Images are 10138: Left eye and right eye folders are separated for each subject. However, there was an overlap of 3 images,
which means left eye folder of a subject had a right eye image or vice versa. SO such images hav been discarded. Therefore total number of images is down to 10,135 now. T
he total number of class folders are 504, out of which, Number of Male folders are 362, Female folders are 142
Number of left eye and right eyes are equal to 252 each. Based on having equal number of left and right eye, we are assuming number of subjects as 252, therefore males are 181, and females are 71
Images have been taken at 5 distances, so the image sizes (h x w) are (401 x 501),(441 x 561), (501 x 651),(651 x 801), (801 x 1001), There are 3 images for each image size.
Therefore each folder has 15 images, if one session. Some of the images have 2 sessions, which means upto 30 images in each folder.

Next we consolidated the left and right eye folder into one folder for that particular subject. This means we now have 252 subject folders. Out of 252, 89 have more than 30 images, which  implies 89 subjects has 2 sessions. Total number of subjects with more than 30 images are 89 and they are ['subj_1', 'subj_100', 'subj_101', 'subj_103', 'subj_113', 'subj_114', 'subj_117', 'subj_118', 'subj_119', 'subj_120', 'subj_122', 'subj_123', 'subj_124', 'subj_127', 'subj_128', 'subj_129', 'subj_130', 'subj_131', 'subj_149', 'subj_155', 'subj_157', 'subj_159', 'subj_160', 'subj_162', 'subj_163', 'subj_165', 'subj_166', 'subj_168', 'subj_172', 'subj_179', 'subj_182', 'subj_183', 'subj_186', 'subj_189', 'subj_190', 'subj_191', 'subj_194', 'subj_195', 'subj_196', 'subj_20', 'subj_201', 'subj_211', 'subj_213', 'subj_217', 'subj_22', 'subj_221', 'subj_222', 'subj_223', 'subj_225', 'subj_226', 'subj_23', 'subj_233', 'subj_234', 'subj_24', 'subj_25', 'subj_251', 'subj_252', 'subj_254', 'subj_255', 'subj_31', 'subj_37', 'subj_4', 'subj_43', 'subj_45', 'subj_46', 'subj_47', 'subj_48', 'subj_5', 'subj_50', 'subj_51', 'subj_52', 'subj_53', 'subj_54', 'subj_55', 'subj_56', 'subj_61', 'subj_63', 'subj_64', 'subj_74', 'subj_75', 'subj_76', 'subj_83', 'subj_86', 'subj_88', 'subj_89', 'subj_92', 'subj_93', 'subj_94', 'subj_95']
 Next, we have separated the images for male and female and different folders (Male_Female_Folders_Images). There are 181 males with total images of 7266 male images and 2869 female images. 
 After this we chose images with width of 561 and 651 and moved it to a different folder for both and women (Male_Female_Training_Folders_Images). We have a total of 2912 male images now and 1146 female images. 
 
 After speaking with Dr. Nasrabadi, we decided to train with only the left eye as per the State-of-the-art. SO, we had to create left and right folder images separately and not consolidate them. ANother thing we discussed was to split the subjects with multiple sessions into two subjects where first session corresponds to one subject and second session will be considered as a different subject. Therefore, we first split the class folders into left and right images using the function "consolidate_left_right". We split the Class_Folders into Class_Folder_Left_Images and Class_Folders_Right_Images. Next we split the subjects with more than one session images or more than 15 images into two different subjects. For this we used the function "split_classes_more_images" and we split the left and right image folders into Class_Folder_Left_Images_Split and Class_Folder_Right_Images_Split. Initially after splitting into multiple subjects, the Right folder split had an extra folder, subj_217_S2, where subj_217_S2 had only one session 2 image,  We had 340 right and 339 left folders. However I just moved back that single image to subj_217 folder and deleted the subj_217_S2 folder. After this we got 339 subject folders each for both left and right eyes. 
 Dr. Nasrabadi also wanted me to use all the image sizes and resize all the image sizes to h x w of 401 x 501. This will be done on the fly in Pytorch.
 
 
 
 UBIRIS_V2 - 11,101 images. We have gone with the setting that is given in DEEP-PRWIS, where we have considered left and right eye as a different subject.
 So, overall, we have 11,101 images from 518 different subjects (259 left and 259 Right). We have used the function create_class_folders from the pre-processing_UBIRIS_V2.py to separate the images into different classes.
 We have stored the separate class folders in the location "/home/n-lab/Documents/Periocular_project/Datasets/UBIris_v2/Class_Folders".
 We have also consolidated the left and right eyes as one subject and stored the result in "/home/n-lab/Documents/Periocular_project/Datasets/UBIris_v2/Consolidated_Folders"
 
 I started to create the attribute.csv for training the model for UBIRIS_V2.
 I used the attribute.csv for UBIPr dataset to generate the attribute csv using the function create_csv_for_attrbiute from pre-processing_UBIRIS_V2.py.
 While running the code I found that there were 14 subject ids ['C520', 'C521', 'C56', 'C519', 'C483', 'C53', 'C484', 'C517', 'C518', 'C428', 'C427', 'C522', 'C54', 'C55']
 that were not present in the attribute.csv from UBIPr.
 I looked at the images in the above folders to see if the subjects were male or female and updated the function to be able to write it into the csv file.
 ['C520', 'C521', 'C519', 'C483', 'C484', 'C517', 'C518', 'C522']  are all males
['C428', 'C427'] are all Females.
 C53, C54 - Bad images. totally different images from what is normally captured for other images. These images are full faces.  So, removed this folder totally from the Class folder
  C55, C56 - Bad images. totally different images from what is normally captured for other images. These images are full faces. So, removed this folder totally from the Class folder

After removing the four folders, the total number of images came down to 514 with total images equal to 11,041.
The new folder after removing the above four folders is saved at "/home/n-lab/Documents/Periocular_project/Datasets/UBIris_v2/Class_Folders_No_Attributes"
This folder will be further used for training the classification model, attribute classification and also joint optimization


 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
